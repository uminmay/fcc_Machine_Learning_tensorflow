{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bf5e8c84-d1b1-479e-b6b6-7a57a2d7f32e",
   "metadata": {},
   "source": [
    "# Q Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30bee646-9878-4411-862b-a5668573424b",
   "metadata": {},
   "source": [
    "Reinforcement learning <-> requires an agent which looks at the enviroment.\n",
    "Agent <-> exploring an enviroment. Say Playing games, i.e. machine playing games.\n",
    "States(say health of agent in a game, mostly location, where the agent is in the environemt) <-> stages <-> position <-> "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86d955e5-28e7-432a-a630-68e19f16c943",
   "metadata": {},
   "source": [
    "- Environemt\n",
    "- Agent\n",
    "- State\n",
    "- Action\n",
    "- Reward (which action maximizes it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a777f6de-bb29-4897-9eec-a525b92319a7",
   "metadata": {},
   "source": [
    "Enviroment Exploraion\n",
    "\n",
    "- Randomly picking a valid action\n",
    "- Using the current Q-Table to find the best action."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fc2fea8-fbaa-4734-892d-131719fe611d",
   "metadata": {},
   "source": [
    "Updating Q-Values\n",
    "\n",
    "$Q[state, action] = Q[state, action] + \\alpha * (reward + \\gamma * max(Q[newState, :]) - Q[state, action])$\n",
    "\n",
    "- $\\alpha$ is the learning rate (how much are we allowed to update Q Value by in each Qtable update).\n",
    "- $\\gamma$ is the discount factor (balance b/w future possible rewards and current)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ab2af6f4-067b-4670-87b7-f285c94dade5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "996c52b2-fb70-4afd-8329-09d3cd4a3d4a",
   "metadata": {},
   "source": [
    "Matrix with column as actions and rows as state, stores reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "584a8cbe-fd66-468d-b320-36ba8133e301",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'gym'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c727ea424a25>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0mgym\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'gym'"
     ]
    }
   ],
   "source": [
    "import gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4994329-b52a-4e97-a071-ba1cd3003056",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
